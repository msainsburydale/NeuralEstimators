estimators, parameters, Z,
use_gpu = use_gpu, verbose = verbose
)
', estimators = estimators, parameters = parameters, Z = Z, use_gpu = TRUE, verbose = TRUE)
juliaLet('
using NeuralEstimators
using Flux
assessment = assess(
estimators, parameters, Z,
use_gpu = use_gpu, verbose = verbose
)
', estimators = list(estimator), parameters = parameters, Z = Z, use_gpu = TRUE, verbose = TRUE)
theta_test
juliaLet('
using NeuralEstimators
using Flux
assessment = assess(
estimators, parameters, Z,
use_gpu = use_gpu, verbose = verbose
)
', estimators = list(estimator), parameters = theta_test, Z = Z, use_gpu = TRUE, verbose = TRUE)
juliaLet('
using NeuralEstimators
using Flux
assessment = assess(
estimators, parameters, Z,
use_gpu = use_gpu, verbose = verbose
)
', estimators = list(estimator), parameters = theta_test, Z = Z_test, use_gpu = TRUE, verbose = TRUE)
juliaLet('
using NeuralEstimators
using Flux
assessment = assess(
estimators, parameters, Z
)
', estimators = list(estimator), parameters = theta_test, Z = Z_test, use_gpu = TRUE, verbose = TRUE)
juliaLet('
using NeuralEstimators
using Flux
#   assessment = assess(
#         estimators, parameters, Z,
# 		    use_gpu = use_gpu, verbose = verbose
# 		  )
# Infer all_m from Z and check that Z is in the correct format
all_m = broadcast.(z -> size(z)[end], Z)
all_m = unique.(all_m)
@assert length(all_m) == length(Z) "The simulated data Z should be a `Vector{Vector{Array}}`, where each `Vector{Array}` is associated with a single sample size (i.e., the size of the final dimension of the arrays should be constant)."
# Check that the number of parameters are consistent with other quantities
K = size(parameters, 2)
KJ = unique(length.(Z))
@assert length(KJ) == 1
KJ = KJ[1]
@assert KJ % K == 0 "The number of data sets in Z must be a multiple of the number of parameters"
J = KJ ÷ K
J > 1 && @info "There are more simulated data sets than unique parameter vectors; ensure that the data are replicated in an fashion, so that the parameter vectors run faster than the replicated data sets."
', estimators = list(estimator), parameters = theta_test, Z = Z_test, use_gpu = TRUE, verbose = TRUE)
juliaLet('
using NeuralEstimators
using Flux
#   assessment = assess(
#         estimators, parameters, Z,
# 		    use_gpu = use_gpu, verbose = verbose
# 		  )
# Infer all_m from Z and check that Z is in the correct format
all_m = broadcast.(z -> size(z)[end], Z)
# all_m = unique.(all_m)
# @assert length(all_m) == length(Z) "The simulated data Z should be a `Vector{Vector{Array}}`, where each `Vector{Array}` is associated with a single sample size (i.e., the size of the final dimension of the arrays should be constant)."
#
# # Check that the number of parameters are consistent with other quantities
# K = size(parameters, 2)
# KJ = unique(length.(Z))
# @assert length(KJ) == 1
# KJ = KJ[1]
# @assert KJ % K == 0 "The number of data sets in Z must be a multiple of the number of parameters"
# J = KJ ÷ K
# J > 1 && @info "There are more simulated data sets than unique parameter vectors; ensure that the data are replicated in an fashion, so that the parameter vectors run faster than the replicated data sets."
', estimators = list(estimator), parameters = theta_test, Z = Z_test, use_gpu = TRUE, verbose = TRUE)
juliaLet('
using NeuralEstimators
using Flux
#   assessment = assess(
#         estimators, parameters, Z,
# 		    use_gpu = use_gpu, verbose = verbose
# 		  )
# Infer all_m from Z and check that Z is in the correct format
# all_m = broadcast.(z -> size(z)[end], Z)
# all_m = unique.(all_m)
# @assert length(all_m) == length(Z) "The simulated data Z should be a `Vector{Vector{Array}}`, where each `Vector{Array}` is associated with a single sample size (i.e., the size of the final dimension of the arrays should be constant)."
#
# # Check that the number of parameters are consistent with other quantities
# K = size(parameters, 2)
# KJ = unique(length.(Z))
# @assert length(KJ) == 1
# KJ = KJ[1]
# @assert KJ % K == 0 "The number of data sets in Z must be a multiple of the number of parameters"
# J = KJ ÷ K
# J > 1 && @info "There are more simulated data sets than unique parameter vectors; ensure that the data are replicated in an fashion, so that the parameter vectors run faster than the replicated data sets."
', estimators = list(estimator), parameters = theta_test, Z = Z_test, use_gpu = TRUE, verbose = TRUE)
all_m  <- c(5, 10, 20, 30, 40, 50, 70, 90, 120, 150)
Z_test <- lapply(all_m, function(m) simulate(theta_test, m))
juliaLet('
using NeuralEstimators
using Flux
#   assessment = assess(
#         estimators, parameters, Z,
# 		    use_gpu = use_gpu, verbose = verbose
# 		  )
# Infer all_m from Z and check that Z is in the correct format
# all_m = broadcast.(z -> size(z)[end], Z)
# all_m = unique.(all_m)
# @assert length(all_m) == length(Z) "The simulated data Z should be a `Vector{Vector{Array}}`, where each `Vector{Array}` is associated with a single sample size (i.e., the size of the final dimension of the arrays should be constant)."
#
# # Check that the number of parameters are consistent with other quantities
# K = size(parameters, 2)
# KJ = unique(length.(Z))
# @assert length(KJ) == 1
# KJ = KJ[1]
# @assert KJ % K == 0 "The number of data sets in Z must be a multiple of the number of parameters"
# J = KJ ÷ K
# J > 1 && @info "There are more simulated data sets than unique parameter vectors; ensure that the data are replicated in an fashion, so that the parameter vectors run faster than the replicated data sets."
', estimators = list(estimator), parameters = theta_test, Z = Z_test, use_gpu = TRUE, verbose = TRUE)
juliaLet('
using NeuralEstimators
using Flux
', estimators = list(estimator), parameters = theta_test, Z = Z_test, use_gpu = TRUE, verbose = TRUE)
estimator <- train(
estimator,
theta_train = theta_train,
theta_val   = theta_val,
Z_train = Z_train,
Z_val   = Z_val,
epochs = 30L
)
unlink(paste0(normalizePath(tempdir()), "/", dir(tempdir())), recursive = TRUE)
estimator <- train(
estimator,
theta_train = theta_train,
theta_val   = theta_val,
Z_train = Z_train,
Z_val   = Z_val,
epochs = 30L
)
dir(tempdir())
tempdir()
dir(tempdir())
```{r}
library("NeuralEstimators")
library("NeuralEstimators")
library("JuliaConnectoR")
library("NeuralEstimators")
library("JuliaConnectoR")
First, we sample parameters from the prior $\Omega(\cdot)$ to construct parameter sets used for training, validating, and testing the estimator. Here, we use the priors $\mu \sim N(0, 1)$ and $\sigma \sim U(0.1, 1)$, and we assume that the parameters are independent a priori. The sampled parameters are stored as $p \times K$ matrices, with $p$ the number of parameters in the model and $K$ the number of sampled parameter vectors.
```{r}
prior <- function(K) {
mu    <- rnorm(K)
sigma <- rgamma(K, 1)
theta <- matrix(c(mu, sigma), byrow = TRUE, ncol = K)
return(theta)
}
theta_train = prior(1000)
theta_val   = prior(200)
theta_test  = prior(100)
simulate <- function(theta_set, m) {
apply(theta_set, 2, function(theta) {
Z <- rnorm(m, theta[1], theta[2])
dim(Z) <- c(1, m)
Z
}, simplify = FALSE)
}
m <- 15
Z_train <- simulate(theta_train, m)
Z_val   <- simulate(theta_val, m)
estimator <- juliaEval('
using NeuralEstimators
using Flux
p = 2    # number of parameters in the statistical model
w = 32   # number of neurons in each layer
psi = Chain(Dense(1, w, relu), Dense(w, w, relu))
phi = Chain(Dense(w, w, relu), Dense(w, p))
estimator = DeepSet(psi, phi)
')
estimator <- train(
estimator,
theta_train = theta_train,
theta_val   = theta_val,
Z_train = Z_train,
Z_val   = Z_val,
epochs = 30L
)
devtools::build(vignettes = F)
install.packages("~/Dropbox/NeuralEstimators_0.0.1.tar.gz", repos = NULL, type = "source")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
library("NeuralEstimators")
library("JuliaConnectoR")
prior <- function(K) {
mu    <- rnorm(K)
sigma <- rgamma(K, 1)
theta <- matrix(c(mu, sigma), byrow = TRUE, ncol = K)
return(theta)
}
theta_train = prior(1000)
theta_val   = prior(200)
theta_test  = prior(100)
simulate <- function(theta_set, m) {
apply(theta_set, 2, function(theta) {
Z <- rnorm(m, theta[1], theta[2])
dim(Z) <- c(1, m)
Z
}, simplify = FALSE)
}
m <- 15
Z_train <- simulate(theta_train, m)
Z_val   <- simulate(theta_val, m)
estimator <- juliaEval('
using NeuralEstimators
using Flux
p = 2    # number of parameters in the statistical model
w = 32   # number of neurons in each layer
psi = Chain(Dense(1, w, relu), Dense(w, w, relu))
phi = Chain(Dense(w, w, relu), Dense(w, p))
estimator = DeepSet(psi, phi)
')
estimator <- train(
estimator,
theta_train = theta_train,
theta_val   = theta_val,
Z_train = Z_train,
Z_val   = Z_val,
epochs = 30L
)
all_m  <- c(5, 10, 20, 30, 40, 50, 70, 90, 120, 150)
Z_test <- lapply(all_m, function(m) simulate(theta_test, m))
juliaLet('
using NeuralEstimators
using Flux
', estimators = list(estimator), parameters = theta_test, Z = Z_test, use_gpu = TRUE, verbose = TRUE)
assessment <- assess(list(estimator), theta_test, Z_test)
parameter_labels <- c("θ1" = expression(mu), "θ2" = expression(sigma))
plotrisk(assessment, parameter_labels = parameter_labels)
plotrisk(assessment$estimates, parameter_labels = parameter_labels)
J     <- 300
theta <- as.matrix(c(0, 0.5))
Z     <- lapply(1:J, function(i) simulate(theta, m))
Z     <- do.call(c, Z)
assessment <- assess(list(estimator), theta, list(Z))
plotdistribution(assessment$estimates, type = "scatter", parameter_labels = parameter_labels)
Z <- simulate(theta, m)        # pretend that this is observed data
estimate(estimator, Z)         # point estimates from the observed data
theta
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::load_all(".")
?plotrisk
devtools::load_all(".")
library("devtools")
document()
?plotrisk
m         <- rep(rep(c(1, 5, 10, 15, 20, 25, 30), each = 50), times = 2)
Z         <- lapply(m, rnorm)
estimate  <- sapply(Z, mean)
df <- data.frame(
estimator = c("Estimator 1", "Estimator 2"),
parameter = "mu", m = m, estimate = estimate, truth = 0
)
plotrisk(df, parameter_labels = c("mu" = expression(mu)))
plotrisk(df)
devtools::load_all(".")
plotrisk(df, parameter_labels = c("mu" = expression(mu)))
document()
?plotrisk
m         <- rep(rep(c(1, 5, 10, 15, 20, 25, 30), each = 50), times = 2)
Z         <- lapply(m, rnorm)
estimate  <- sapply(Z, mean)
df <- data.frame(
estimator = c("Estimator 1", "Estimator 2"),
parameter = "mu", m = m, estimate = estimate, truth = 0
)
parameter_labels <- c("mu" = expression(mu))
# Plot the risk function
plotrisk(df, parameter_labels = parameter_labels)
plotrisk(df, loss = function(x, y) (x-y)^2)
plotrisk(df)
#' Z         <- lapply(m, rnorm)
#' estimate  <- sapply(Z, mean)
#' df <- data.frame(
#'   estimator = c("Estimator 1", "Estimator 2"),
#'   parameter = "mu", m = m, estimate = estimate, truth = 0
#' )
#'
#' # Plot the risk function
#' plotrisk(df)
#' plotrisk(df, loss = function(x, y) (x-y)^2)
plotrisk <- function(df, parameter_labels = NULL, loss = function(x, y) abs(x - y)) {
df <- df %>% mutate(residual = estimate - truth)
if (is.null(parameter_labels)) {
param_labeller <- identity
} else {
param_labeller <- label_parsed
df <- mutate_at(df, .vars = "parameter", .funs = factor, levels = names(parameter_labels), labels = parameter_labels)
}
# Compute global risk for each combination of estimator and sample size m
df <- df %>%
group_by(estimator, parameter, m) %>%
dplyr::summarise(loss = mean(loss(estimate, truth)))
# Plot risk vs. m
gg <- ggplot(data = df, aes(x = m, y = loss, colour = estimator, group = estimator)) +
geom_point() +
geom_line() +
facet_wrap(parameter ~ ., scales = "free", labeller = param_labeller) +
labs(colour = "", x = expression(m), y = expression(r[Omega](hat(theta)))) +
theme_bw() +
theme(legend.text.align = 0,
panel.grid = element_blank(),
strip.background = element_blank())
return(gg)
}
?plotruntime
m  <- rep(c(1, 5, 10, 15, 20, 25, 30), each = 2)
df <- data.frame(
estimator = c("Estimator 1", "Estimator 2"),
m = m,
time = 1 / m + 0.1 * runif(length(m))
)
plotruntime(df)
legend
devtools::load_all(".")
document()
?plotdistribution
# In the following, we have two estimators and, for each parameter, 50 estimates
# from each estimator.
# Single parameter:
estimators <- c("Estimator 1", "Estimator 2")
df <- data.frame(
estimator = estimators, truth = 0, parameter = "mu",
estimate  = rnorm(2*50),
replicate = rep(1:50, each = 2)
)
parameter_labels <- c("mu" = expression(mu))
estimator_labels <- c("Estimator 1" = expression(hat(theta)[1]("·")),
"Estimator 2" = expression(hat(theta)[2]("·")))
plotdistribution(df, parameter_labels = parameter_labels, estimator_labels = estimator_labels)
plotdistribution(df, parameter_labels = parameter_labels, type = "density")
# Two parameters:
df <- rbind(df, data.frame(
estimator = estimators, truth = 1, parameter = "sigma",
estimate  = rgamma(2*50, shape = 1, rate = 1),
replicate = rep(1:50, each = 2)
))
parameter_labels <- c(parameter_labels, "sigma" = expression(sigma))
plotdistribution(df, parameter_labels = parameter_labels)
plotdistribution(df, parameter_labels = parameter_labels, type = "density")
plotdistribution(df, parameter_labels = parameter_labels, type = "scatter")
# Three parameters:
df <- rbind(df, data.frame(
estimator = estimators, truth = 0.25, parameter = "alpha",
estimate  = 0.5 * runif(2*50),
replicate = rep(1:50, each = 2)
))
parameter_labels <- c(parameter_labels, "alpha" = expression(alpha))
plotdistribution(df, parameter_labels = parameter_labels)
plotdistribution(df, parameter_labels = parameter_labels, type = "density")
plotdistribution(df, parameter_labels = parameter_labels, type = "scatter")
plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", pairs = TRUE)
# Pairs plot with user-specified plots in the upper triangle:
upper_triangle_plots <- lapply(1:3, function(i) {
x = rnorm(10)
y = rnorm(10)
shape = sample(c("Class 1", "Class 2"), 10, replace = TRUE)
qplot(x = x, y = y, shape = shape) +
labs(shape = "") +
theme_bw()
})
plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", pairs = TRUE,
upper_triangle_plots = upper_triangle_plots)
plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", pairs = TRUE, legend = FALSE)
plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", pairs = TRUE, upper_triangle_plots = upper_triangle_plots, legend = FALSE)
devtools::build(vignettes = F)
devtools::build(vignettes = F)
install.packages("~/Dropbox/NeuralEstimators_0.0.1.tar.gz", repos = NULL, type = "source")
devtools::load_all(".")
?plotdistribution
# Single parameter:
estimators <- c("Estimator 1", "Estimator 2")
df <- data.frame(
estimator = estimators, truth = 0, parameter = "mu",
estimate  = rnorm(2*50),
replicate = rep(1:50, each = 2)
)
parameter_labels <- c("mu" = expression(mu))
estimator_labels <- c("Estimator 1" = expression(hat(theta)[1]("·")),
"Estimator 2" = expression(hat(theta)[2]("·")))
plotdistribution(df, parameter_labels = parameter_labels, estimator_labels = estimator_labels)
plotdistribution(df, parameter_labels = parameter_labels, type = "density")
# Two parameters:
df <- rbind(df, data.frame(
estimator = estimators, truth = 1, parameter = "sigma",
estimate  = rgamma(2*50, shape = 1, rate = 1),
replicate = rep(1:50, each = 2)
))
parameter_labels <- c(parameter_labels, "sigma" = expression(sigma))
plotdistribution(df, parameter_labels = parameter_labels)
plotdistribution(df, parameter_labels = parameter_labels, type = "density")
plotdistribution(df, parameter_labels = parameter_labels, type = "scatter")
library(ggExtra)
install.packages("ggExtra")
library("ggExtra")
?ggMarginal
p <- plotdistribution(df, parameter_labels = parameter_labels, type = "scatter")
ggMarginal(p)
ggMarginal(p[[1]])
ggMarginal(p[[1]], groupColour = TRUE)
devtools::load_all(".")
p <- plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", truth_lines = TRUE)
p
devtools::load_all(".")
p <- plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", truth_lines = TRUE)
devtools::load_all(".")
p <- plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", truth_lines = TRUE)
?geom_vline
ggplot(data = df[sample(nrow(df)), ]) +
geom_point(
aes_string(
x = paste("estimate", p[1], sep = "_"),
y = paste("estimate", p[2], sep = "_"),
colour = "estimator"
),
alpha = 0.75) +
geom_point(
aes_string(
x = paste("truth", p[1], sep = "_"),
y = paste("truth", p[2], sep = "_")
),
colour = truth_colour, shape = "+", size = truth_size
) +
geom_vline(
aes_string(xintercept = paste("truth", p[1], sep = "_")),
colour = truth_colour, size = truth_size
) +
labs(colour = "", x = parameter_labels[[p[1]]], y = parameter_labels[[p[2]]]) +
scale_colour_viridis(discrete = TRUE, labels = estimator_labels) +
theme_bw()
ggplot(data = df[sample(nrow(df)), ]) +
geom_point(
aes_string(
x = paste("estimate", p[1], sep = "_"),
y = paste("estimate", p[2], sep = "_"),
colour = "estimator"
),
alpha = 0.75) +
geom_point(
aes_string(
x = paste("truth", p[1], sep = "_"),
y = paste("truth", p[2], sep = "_")
),
colour = truth_colour, shape = "+", size = truth_size
) +
geom_vline(
aes_string(xintercept = paste("truth", p[1], sep = "_")),
colour = truth_colour
) +
labs(colour = "", x = parameter_labels[[p[1]]], y = parameter_labels[[p[2]]]) +
scale_colour_viridis(discrete = TRUE, labels = estimator_labels) +
theme_bw()
# Generate the scatterplot estimation panels
scatterplots <- apply(combinations, 2, function(p) {
browser()
ggplot(data = df[sample(nrow(df)), ]) +
geom_point(
aes_string(
x = paste("estimate", p[1], sep = "_"),
y = paste("estimate", p[2], sep = "_"),
colour = "estimator"
),
alpha = 0.75) +
geom_point(
aes_string(
x = paste("truth", p[1], sep = "_"),
y = paste("truth", p[2], sep = "_")
),
colour = truth_colour, shape = "+", size = truth_size
) +
geom_vline(aes_string(xintercept = paste("truth", p[1], sep = "_")), colour = truth_colour) +
geom_hline(aes_string(yintercept = paste("truth", p[2], sep = "_")), colour = truth_colour) +
labs(colour = "", x = parameter_labels[[p[1]]], y = parameter_labels[[p[2]]]) +
scale_colour_viridis(discrete = TRUE, labels = estimator_labels) +
theme_bw()
})
devtools::load_all(".")
p <- plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", truth_lines = TRUE)
p
devtools::load_all(".")
p <- plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", truth_line_size = 4)
ggMarginal(p[[1]], groupColour = TRUE)
p <- plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", truth_line_size = NULL)
ggMarginal(p[[1]], groupColour = TRUE)
devtools::load_all(".")
p <- plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", truth_line_size = NULL)
ggMarginal(p[[1]], groupColour = TRUE)
p <- plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", truth_line_size = 1)
ggMarginal(p[[1]], groupColour = TRUE)
p <- plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", truth_line_size = 0.5)
ggMarginal(p[[1]], groupColour = TRUE)
p <- plotdistribution(df, parameter_labels = parameter_labels, type = "scatter", truth_line_size = 1)
ggMarginal(p[[1]], groupColour = TRUE)
devtools::build(vignettes = F)
