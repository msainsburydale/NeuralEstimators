---
title: "NeuralEstimators"
author: "Matthew Sainsbury-Dale, Andrew Zammit-Mangion, and Raphaël Huser"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{NeuralEstimators}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

Neural estimators are neural networks that transform data into parameter point estimates, and they are a promising recent approach to inference. They are likelihood free, substantially faster than classical methods, and can be designed to be approximate Bayes estimators.  Uncertainty quantification with neural estimators is also straightforward through the bootstrap distribution, which is essentially available "for free" with a neural estimator.

The package `NeuralEstimators` facilitates the development of neural estimators in a user-friendly manner. It caters for arbitrary models by having the user implicitly define their model via simulated data. This makes the development of neural estimators particularly straightforward for models with existing implementations. The documentation for the native `Julia` version of the package is available [here](https://msainsburydale.github.io/NeuralEstimators.jl/dev/). 

Below, we outline the theoretical framework of neural estimation, and then provide an example of how a neural estimator may be easily constructed using the package `NeuralEstimators`.


## Methodology

In this section, we provide an overview of point estimation using neural Bayes estimators. For a more detailed discussion on the framework and its implementation, see Sainsbury-Dale, Zammit-Mangion, and Huser (2023), "Likelihood-free parameter estimation with neural Bayes estimators", The American Statistician, DOI:
10.1080/00031305.2023.2249522.

### Neural Bayes estimators

A parametric statistical model is a set of probability distributions on a sample space $\mathcal{S}$, where the probability distributions are parameterised via some $p$-dimensional parameter vector $\boldsymbol{\theta}$ on a parameter space $\Theta$. Suppose that we have data from one such distribution, which we denote as $\boldsymbol{Z}$. Then, the goal of parameter point estimation is to come up with an estimate of the unknown $\boldsymbol{\theta}$ from $\boldsymbol{Z}$ using an estimator,
 $$
 \hat{\boldsymbol{\theta}} : \mathcal{S} \to \Theta,
$$
which is a mapping from the sample space to the parameter space.

Estimators can be constructed within a decision-theoretic framework. Assume that the sample space is $\mathcal{S} = \mathbb{R}^n$, and consider a non-negative loss function, $L(\boldsymbol{\theta}, \hat{\boldsymbol{\theta}}(\boldsymbol{Z}))$, which assesses an estimator $\hat{\boldsymbol{\theta}}(\cdot)$ for a given $\boldsymbol{\theta}$ and data set $\boldsymbol{Z} \sim f(\boldsymbol{z} \mid \boldsymbol{\theta})$, where $f(\boldsymbol{z} \mid \boldsymbol{\theta})$ is the probability density function of the data conditional on $\boldsymbol{\theta}$. $\boldsymbol{\theta}$. An estimator's risk function is its loss averaged over all possible data realisations, 
 $$
 R(\boldsymbol{\theta}, \hat{\boldsymbol{\theta}}(\cdot)) \equiv \int_{\mathcal{S}}  L(\boldsymbol{\theta}, \hat{\boldsymbol{\theta}}(\boldsymbol{z}))f(\boldsymbol{z} \mid \boldsymbol{\theta}) \rm{d} \boldsymbol{z}.
 $$ 
So-called Bayes estimators minimise the Bayes risk, 
$$
 r_{\Omega}(\hat{\boldsymbol{\theta}}(\cdot)) 
 \equiv \int_\Theta R(\boldsymbol{\theta}, \hat{\boldsymbol{\theta}}(\cdot)) \rm{d} \Omega(\boldsymbol{\theta}),  
 $$ 
where $\Omega(\cdot)$ is a prior measure for $\boldsymbol{\theta}$. 

Bayes estimators are theoretically attractive: for example, unique Bayes estimators are admissible and, under suitable regularity conditions and the squared-error loss, are consistent and asymptotically efficient. Further, for a large class of prior distributions, every set of conditions that imply consistency of the maximum likelihood (ML) estimator also imply consistency of Bayes estimators. Importantly, Bayes estimators are not motivated purely by asymptotics: by construction, they are Bayes irrespective of the sample size and model class. Unfortunately, however, Bayes estimators are typically unavailable in closed form for the complex models often encountered in practice. A way forward is to assume a flexible parametric model for $\hat{\boldsymbol{\theta}}(\cdot)$, and to optimise the parameters within that model in order to approximate the Bayes estimator. Neural networks are ideal candidates, since they are universal function approximators, and because they are also fast to evaluate, usually involving only simple matrix-vector operations.
 
Let $\hat{\boldsymbol{\theta}}(\boldsymbol{Z}; \boldsymbol{\gamma})$ denote a *neural point estimator*, that is, a neural network that returns a point estimate from data $\boldsymbol{Z}$, where $\boldsymbol{\gamma}$ contains the neural-network parameters. Bayes estimators may be approximated with $\hat{\boldsymbol{\theta}}(\cdot; \boldsymbol{\gamma}^*)$ by solving the optimisation problem,  
$$
\boldsymbol{\gamma}^*
\equiv 
\underset{\boldsymbol{\gamma}}{\mathrm{arg\,min}} \; r_{\Omega}(\hat{\boldsymbol{\theta}}(\cdot; \boldsymbol{\gamma})). 
$$ 
 Typically, $r_{\Omega}(\cdot)$ cannot be directly evaluated, but it can be approximated using Monte Carlo methods. Specifically, given a set of $K$ parameter vectors sampled from the prior $\Omega(\cdot)$ denoted by $\vartheta$ and, for each $\boldsymbol{\theta} \in \vartheta$, $J$ realisations from $f(\boldsymbol{z} \mid  \boldsymbol{\theta})$ collected in $\mathcal{Z}_{\boldsymbol{\theta}}$, 
 $$
 r_{\Omega}(\hat{\boldsymbol{\theta}}(\cdot; \boldsymbol{\gamma})) 
 \approx 
\frac{1}{K} \sum_{\boldsymbol{\theta} \in \vartheta} \frac{1}{J} \sum_{\boldsymbol{z} \in \mathcal{Z}_{\boldsymbol{\theta}}} L(\boldsymbol{\theta}, \hat{\boldsymbol{\theta}}(\boldsymbol{z}; \boldsymbol{\gamma})).  
 $$ 
 Note that the above approximation does not involve evaluation, or knowledge, of the likelihood function. 
 
 The Monte-Carlo-approximated Bayes risk can be straightforwardly minimised with respect to $\boldsymbol{\gamma}$ using back-propagation and stochastic gradient descent. For sufficiently flexible architectures, the point estimator targets a Bayes estimator with respect to $L(\cdot, \cdot)$ and $\Omega(\cdot)$. We therefore call the fitted neural point estimator a  *neural Bayes estimator*. Like Bayes estimators, neural Bayes estimators target a specific point summary of the posterior distribution. For instance, the absolute-error and squared-error loss functions lead to neural Bayes estimators that approximate the posterior median and mean, respectively.

### Construction of neural Bayes estimators

The neural Bayes estimators is conceptually simple and can be used in a wide range of problems where other approaches, such as maximum-likelihood estimation, are computationally infeasible. The estimator also has marked practical appeal, as the general workflow for its construction is only loosely connected to the statistical or physical model being considered. The workflow is as follows:

  1. Define the prior, $\Omega(\cdot)$. 
  1. Choose a loss function, $L(\cdot, \cdot)$, typically the absolute-error or squared-error loss. 
  1. Design a suitable neural-network architecture for the neural point estimator $\hat{\boldsymbol{\theta}}(\cdot; \boldsymbol{\gamma})$. 
  1. Sample parameters from $\Omega(\cdot)$ to form training/validation/test parameter sets. 
  1. Given the above parameter sets, simulate data from the model, to form training/validation/test data sets. 
  1. Train the neural network (i.e., estimate $\boldsymbol{\gamma}$) by minimising the loss function averaged over the training sets. During training, monitor performance and convergence using the validation sets.
  1. Assess the fitted neural Bayes estimator, $\hat{\boldsymbol{\theta}}(\cdot; \boldsymbol{\gamma}^*)$, using the test set. 


## Package overview

Some general points to keep in mind when working with the package.  

- Parameters sampled from the prior distribution should be stored as $p\times K$ matrices, where $p$ is the number of parameters in the statistical model and $K$ is the number of sampled parameter vectors. 
- The simulated data should be stored as a list, where each element of the list corresponds to a data set simulated conditional on one parameter vector. 
- The format of the data (i.e., each element of the list described above) depends on the architecture of the neural estimator, which in turn is dictated by the structure of the response variable. For example, if the data are spatial and measured on a regular grid, the estimator will be based on a convolutional neural network (CNN), and each data set is stored as a four-dimensional array, with the first three dimensions corresponding to the width, height, and depth/channels dimensions, and the fourth dimension storing the independent replicates. For unstructured $d$-dimensional data, the estimator will be based on a dense neural network (DNN), and each data set is stored as a two-dimensional array (a matrix), with the second dimension storing the independent replicates. 

The examples given below serve as a good introduction to the package and further illustrate the points above. 


## Examples

### Univariate data

Here, we develop a neural Bayes estimator for $\boldsymbol{\theta} \equiv (\mu, \sigma)'$ from data $Z_1, \dots, Z_m$ that are independent and identically distributed according to a $\rm{N}(\mu, \sigma^2)$ distribution.

Before proceeding, we load the required packages. The package `JuliaConnectoR` is used to call Julia from R. 

```{r}
library("NeuralEstimators")
library("JuliaConnectoR")
library("ggplot2")
library("ggpubr")
```


First, we sample parameters from the prior $\Omega(\cdot)$ to construct parameter sets used for training, validating, and testing the estimator. Here, we use the priors $\mu \sim \rm{N}(0, 1)$ and $\sigma \sim \rm{Gamma}(1, 1)$, and we assume that the parameters are independent a priori. The sampled parameters are stored as $p \times K$ matrices, with $p$ the number of parameters in the model and $K$ the number of sampled parameter vectors.

```{r}
prior <- function(K) {
  mu    <- rnorm(K)
  sigma <- rgamma(K, 1)
  theta <- matrix(c(mu, sigma), byrow = TRUE, ncol = K)
  return(theta)
}
set.seed(1)
theta_train = prior(10000) 
theta_val   = prior(1000)
```

Next, we implicitly define the statistical model with simulated data. The data are stored as a `list`, where each element of the list is associated with one parameter vector. Since our data is replicated, we will use the Deep Sets framework and, since each replicate is univariate, we will use a dense neural network (DNN) for the inner network. Since the inner network is a DNN, each element of the list should be an `array`, with the independent replicates stored in the final dimension.

```{r}
simulate <- function(theta_set, m) {
  apply(theta_set, 2, function(theta) {
   Z <- rnorm(m, theta[1], theta[2])
   dim(Z) <- c(1, m) 
   Z
  }, simplify = FALSE)
}

m <- 15
Z_train <- simulate(theta_train, m)
Z_val   <- simulate(theta_val, m)
```


We now design architectures for the inner and outer neural networks, $\mathbf{\psi}(\cdot)$ and $\mathbf{\phi}(\cdot)$ respectively, in the Deep Set framework, and initialise the neural estimator as a `DeepSet` object. Since we have univariate data, we use a dense neural network with one input neuron. This is the only stage of the workflow that requires the user to write Julia code, which is executed using the function `JuliaEval`.


```{r, results='hide', message=FALSE}
estimator <- juliaEval('
  using NeuralEstimators
  using Flux

  p = 2    # number of parameters in the statistical model
  w = 32   # number of neurons in each layer

  psi = Chain(Dense(1, w, relu), Dense(w, w, relu), Dense(w, w, relu))
  phi = Chain(Dense(w, w, relu), Dense(w, p))
  estimator = DeepSet(psi, phi)
')
```

Next, we train the neural estimator using `train()`, here using the default absolute-error loss function. 

```{r}
estimator <- train(
  estimator,
  theta_train = theta_train,
  theta_val   = theta_val,
  Z_train = Z_train,
  Z_val   = Z_val,
  epochs = 50
  )
```

The function `train()` contains an argument `savepath`: if it is specified, the neural-network parameters (i.e., the weights and biases) will be saved during training as `bson` files; the risk function evaluated over the training and validation sets will also be saved, in the first and second columns of `loss_per_epoch.csv`, respectively. The best set of parameters (as measured by validation risk) will be saved as `best_network.bson`: to load these best weights into a neural estimator in a later `R` session, use the function `loadbestweights()`. 

To assess the accuracy of the resulting neural Bayes estimator, we use the function `assess()`. 

```{r}
theta_test <- prior(1000)
Z_test     <- simulate(theta_test, m)
assessment <- assess(estimator, theta_test, Z_test)
head(assessment$estimates)
```

Given a dataframe of estimates and true value returned by a call to `assess()`, the function `risk()` can be used to compute a Monte Carlo approximation of the Bayes risk: 

```{r}
risk(assessment$estimates, average_over_parameters = F)
```


In addition to assessing the estimator with respect to many parameter configurations, it is often helpful to visualise the empirical sampling distribution of an estimator for a particular parameter configuration. This can be done by calling `assess()` with $J$ data sets simulated under a single parameter configuration, and providing the resulting estimates to `plotdistribution()`: 

```{r, results='hide',fig.keep='all', fig.width=6, fig.height=3, fig.align='center'}
J     <- 100
theta <- as.matrix(c(0, 0.5))
Z     <- lapply(1:J, function(i) simulate(theta, m))
Z     <- do.call(c, Z)
assessment <- assess(estimator, theta, Z)

parameter_labels <- c("θ1" = expression(mu), "θ2" = expression(sigma))
joint <- plotdistribution(assessment$estimates, type = "scatter", parameter_labels = parameter_labels)
marginal <- plotdistribution(assessment$estimates, type = "box", parameter_labels = parameter_labels, return_list = TRUE)
ggarrange(plotlist = c(joint, marginal), nrow = 1, common.legend = TRUE)
```



Once the neural Bayes estimator has been trained and assessed, it can be applied to observed data using the function `estimate()`, and compute non-parametric bootstrap estimates using the function `bootstrap()`. Below, we use simulated data as a surrogate for observed data:

```{r}
theta    <- as.matrix(c(0, 0.5))     # true parameters
Z        <- simulate(theta, m)       # pretend that this is observed data
thetahat <- estimate(estimator, Z)   # point estimates from the "observed data"
bs <- bootstrap(estimator, Z)        # non-parametric bootstrap estimates
bs[, 1:6]
```



### Multivariate data

Suppose now that our data consists of $m$ replicates of a $d$-dimensional multivariate distribution. Everything remains as given in the univariate example above, except that we now store the data as a list of $d \times m$ matrices (previously they were stored as $1\times m$ matrices), and the inner network of the DeepSets representation takes a $d$-dimensional input (previously it took a 1-dimensional input). 

For example, to develop a neural Bayes estimator for $\boldsymbol{\theta} \equiv (\mu_1, \mu_2, \sigma, \rho)'$ from data $\mathbf{Z}_1, \dots, \mathbf{Z}_m$ that are independent and identically distributed according to a multivariate $\rm{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ distribution, one may use the following code for defining the prior distribution, the simulator, and the neural-estimator architecture:


```{r, eval = FALSE}
prior <- function(K) {
  mu1    <- rnorm(K)
  mu2    <- rnorm(K)
  sigma  <- rgamma(K, 1)
  rho    <- runif(K, -1, 1)
  theta  <- matrix(c(mu1, mu2, sigma, rho), byrow = TRUE, ncol = K)
  return(theta)
}

library("MASS")
simulate <- function(theta_set, m) {
  apply(theta_set, 2, function(theta) {
    mu    <- c(theta[1], theta[2])
    sigma <- theta[3]
    rho   <- theta[4]
    Sigma <- sigma^2 * matrix(c(1, rho, rho, 1), 2, 2)
    Z <- mvrnorm(m, mu, Sigma)
    Z <- t(Z)      # convert to dxm matrix
    Z
  }, simplify = FALSE)
}

estimator <- juliaEval('
  using NeuralEstimators
  using Flux

  p = 2    # number of parameters in the statistical model
  w = 32   # number of neurons in each layer
  d = 2    # dimension of the response variable

  psi = Chain(Dense(d, w, relu), Dense(w, w, relu), Dense(w, w, relu))
  phi = Chain(Dense(w, w, relu), Dense(w, p))
  estimator = DeepSet(psi, phi)
')
```

Note that, when estimating a covariance matrix, one may wish to constrain the neural estimator to only produce parameters that imply a valid (i.e., positive definite) covariance matrix. This can be achieved by appending a  [CovarianceMatrix](https://msainsburydale.github.io/NeuralEstimators.jl/dev/API/architectures/#NeuralEstimators.CovarianceMatrix) layer to the end of the outer network `phi` above. However, this is often unnecessary as the estimator will typically learn to provide valid estimates, even if not constrained to do so. 


### Gridded spatial data

For spatial data measured on a regular grid, the estimator is based on a convolutional neural network (CNN), and each data set is stored as a four-dimensional array, with the first three dimensions corresponding to width, height, and depth/channels dimensions, and the fourth dimension storing independent replicates. Note that, for univariate spatial processes, the channels dimension is simply equal to 1.

Below, we give example code for a $16\times 16$ spatial grid, and where the spatial model is Schlather's max-stable process with unknown range and smoothness parameters. 


```{r, eval = FALSE}

prior <- function(K) {
  range      <- runif(K, 1, 10)
  smoothness <- runif(K, 0.5, 3)
  theta      <- matrix(c(range, smoothness), byrow = TRUE, ncol = K)
  return(theta)
}

coord <- cbind(1:16, 1:16)
simulate <- function(theta_set, m) {
  apply(theta_set, 2, function(theta) {
    range  <- theta[1]
    smooth <- theta[2]
    Z <- SpatialExtremes::rmaxstab(
      m, coord, cov.mod = "whitmat", grid = TRUE, nugget = 0, 
      range = range, smooth = smooth
      )
    dim(Z) <- c(dim(Z)[1], dim(Z)[2], 1, dim(Z)[3]) 
    Z
  }, simplify = FALSE)
}


estimator <- juliaEval('
  using NeuralEstimators
  using Flux

  p = 2    # number of parameters in the statistical model
  w = 32   # number of neurons in each layer
  d = 2    # dimension of the response variable

  psi = Chain(
		Conv((10, 10), 1 => 64,  relu),
		Conv((5, 5),  64 => 128,  relu),
		Conv((3, 3),  128 => 256, relu),
		Flux.flatten
		)
		
  phi = Chain(
		Dense(256, 500, relu),
		Dense(500, p)
	)
	
  estimator = DeepSet(psi, phi)
')
```



### Irregular spatial data

Coming soon! (Using Graph Neural Networks.)

